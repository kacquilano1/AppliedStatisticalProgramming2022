---
title: "Applied Statistical Programming - Spring 2022"
author: "Kimberly Acquilano"
output: pdf_document
---

```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}
{\Large{\textbf{Problem Set 4}}} \\
\vspace{4 bp}
Due Wednesday, March 23, 10:00 AM (Before Class) \\
\end{center}

\section*{Instructions}
\begin{enumerate}
  \item The following questions should each be answered within an Rmarkdown file. Be sure to provide many comments in your code blocks to facilitate grading. Undocumented code will not be graded.
  \item Work on git. Continue to work in the repository you forked from \url{https://github.com/johnsontr/AppliedStatisticalProgramming2022} and add your code for Problem Set 4. Commit and push frequently. Use meaningful commit messages because these will affect your grade.
  \item You may work in teams, but each student should develop their own Rmarkdown file. To be clear, there should be no copy and paste. Each keystroke in the assignment should be your own.
  \item For students new to programming, this may take a while. Get started.
\end{enumerate}

\section*{\texttt{tidyverse}}

Your task in this problem set is to combine two datasets in order to observe how many endorsements each candidate received using only \texttt{dplyr} functions. Use the same Presidential primary polls that were used for the in class worksheets on February 28 and March 2.


First, create two new objects \texttt{polls} and \texttt{Endorsements}. Then complete the following.
\begin{itemize}
  \item Change the \texttt{Endorsements} variable name endorsee to \texttt{candidate\_name}.
  \item Change the \texttt{Endorsements} dataframe into a \texttt{tibble} object.
  \item Filter the \texttt{poll} variable to only include the following 6 candidates: Amy Klobuchar, Bernard Sanders,Elizabeth Warren, Joseph R. Biden Jr., Michael Bloomberg, Pete Buttigieg \textbf{and} subset the dataset to the following five variables: \texttt{candidate\_name, sample\_size, start\_date, party, pct}
  \item Compare the candidate names in the two datasets and find instances where the a candidates name is spelled differently i.e. Bernard vs. Bernie. Using only \texttt{dplyr} functions, make these the same across datasets. 
  \item Now combine the two datasets by candidate name using \texttt{dplyr} (there will only be five candidates after joining).
  \item Create a variable which indicates the number of endorsements for each of the five candidates using \texttt{dplyr}.
  \item Plot the number of endorsement each of the 5 candidates have using \texttt{ggplot()}. Save your plot as an object \texttt{p}.
  \item Rerun the previous line as follows: \texttt{p + theme\_dark()}. Notice how you can still customize your plot without rerunning the plot with new options.
  \item Now, using the knowledge from the last step change the label of the X and Y axes to be more informative, add a title. Save the plot in your forked repository.
\end{itemize}



```{r}
# Change eval=FALSE in the code block. Install packages as appropriate.
#install.packages("fivethirtyeight")
library(fivethirtyeight)
library(tidyverse)
# URL to the data that you've used.
url <- 'https://jmontgomery.github.io/PDS/Datasets/president_primary_polls_feb2020.csv'
polls <- read_csv(url)
Endorsements <- endorsements_2020 # from the fiverthirtyeight package

Endorsements <- rename(Endorsements, candidate_name = endorsee)
Endorsements <- as_tibble(Endorsements)

polls <- polls %>% 
 filter(candidate_name %in% c("Amy Klobuchar", "Bernard Sanders", "Elizabeth Warren", "Joseph R. Biden Jr.", "Michael Bloomberg", "Pete Buttigieg")) %>% 
  select(candidate_name, sample_size, start_date, party, pct)

#Use summarise to get a list of all candidate names in Endorsements
#Use distinct to only return unique values
Endorsements %>% 
  distinct(candidate_name) %>% 
  summarise(candidate_name)


#polls uses:
#Amy Klobuchar
#Bernard Sanders
#Elizabeth Warren
#Joseph R. Biden Jr.
#Michael Bloomberg
#Pete Buttigieg

#Endorsements uses:
#Joe Biden
#Bernie Sanders
#Amy Klobuchar
#Elizabeth Warren
#Pete Buttigieg
#Michael Bloomberg is missing

#Match names in polls to be those in Endorsements
#Amy Klobuchar, Elizabeth Warrent, and Pete Buttigieg already match
#Replace Bernard Sanders in polls to be Bernie Sanders
#Use mutate function
polls <- polls %>% 
  mutate(candidate_name = replace(candidate_name, candidate_name == "Bernard Sanders", "Bernie Sanders")) 
#Replace Joseph R. Biden Jr. in polls to be Joe Biden
#Use mutate function
polls <- polls %>% 
  mutate(candidate_name = replace(candidate_name, candidate_name == "Joseph R. Biden Jr.", "Joe Biden")) 

#Combine the two datasets
#Use inner_join to combine all matching rows, according to candidate name, and drop rows that do not match
#Create a new tibble for this combo set
newPoll <- polls %>% 
  inner_join(Endorsements, by = "candidate_name")

#Verify there are only 5 candidate names, and that they are the correct names
newPoll %>% 
  distinct(candidate_name) %>% 
  summarise(candidate_name)

#Create an new tibble
#Columns will be candidate_name and number of endorsements (count of number of distinct endorers), called nendorse
groupednewPoll <- newPoll %>% 
  group_by(candidate_name) %>% 
    summarise(nendorse = n_distinct(endorser))

#Create a plot called c
#Plot groupednewPoll data
#Candidate name on x-axis
#Number of endorsements on y-axis
p <- ggplot(groupednewPoll , aes(x = candidate_name, y = nendorse)) +
             #Use geom shape columns
              geom_col()
#Print plot p
p

#Add the dark theme to p
p + theme_dark()

#Customize plot p
#load a new color palatte
#install.packages("viridis")
library(viridis)
#Use plot p
p <- p +
  #Add in column labels, to show the number of endorsements on top of each column
  geom_text(aes(label = nendorse, vjust = -0.2))+
  #Color each column, using the viridis color palette (5 colors from the inferno option)
  geom_col(aes(fill = inferno(5))) +
  #Change x-axis label to "Candidates"
  xlab("Candidates") +
  #Change y-axis label to "Number of Endorsements"
  ylab("Number of Endorsements") +
  #Remove color legend
  guides(fill = FALSE)
#Print new plot p
p

```

$$\\[.1in]$$
$$--------------------------------------------------------$$
$$--------------------------------------------------------$$
$$\\[.1in]$$
Text-as-Data with \texttt{tidyverse}

For this question you will be analyzing Tweets from President Trump for various characteristics. Load in the following packages and data:

```{r}
# Change eval=FALSE in the code block. Install packages as appropriate.
library(tidyverse)
#install.packages('tm')
library(tm) 
#install.packages('lubridate')
library(lubridate)
#install.packages('wordcloud')
library(wordcloud)
trump_tweets_url <- 'https://politicaldatascience.com/PDS/Datasets/trump_tweets.csv'
tweets <- read_csv(trump_tweets_url)
```


  
First separate the \texttt{created\_at} variable into two new variables where the date and the time are in separate columns. After you do that, then report the range of dates that is in this dataset.
```{r}

#Extract date and create new variable,
#Use created_at, which is originally written as month/date/year
tweets$date <- as.Date(tweets$created_at, "%m/%d/%y")

#Extract time and create new variable, starting at fifth to last character and ending at last character of created_at
tweets$times <- str_sub(tweets$created_at, start = -5, end = -1)
#Turn times into time of day, with hours and minutes
tweets$times <- hm(tweets$times)

#get range of dates
range(tweets$date)
```
Range of dates are: 01-01-2020 to 12-31-2020
$$\\[.1in]$$
$$--------------------------------------------------------$$
$$\\[.1in]$$

Using \texttt{dplyr} subset the data to only include original tweets (remove retweents) and show the text of the President's \textbf{top 5} most popular and most retweeted tweets. (Hint: The \texttt{match} function can help you find the index once you identify the largest values.) 
```{r}
#create a new dataframe, a subset of tweets
#include only tweets, no retweets
tweets_only <- tweets[tweets$is_retweet == FALSE,]

#reorder the dataframe according to retweet_count, with highest at the top, put NAs at the end
tweets_only$retweet_count <- sort(tweets_only$retweet_count, decreasing = TRUE, na.last = TRUE)
#save top 5 tweets, the 5 tweets with highest retweet count
most_retweets <- head(tweets_only$text, 5)

#reorder the dataframe according to favorite_count, with the highest at the top, put NAs at the end
tweets_only$favorite_count <- sort(tweets_only$favorite_count, decreasing = TRUE, na.last = TRUE)
#save top 5 tweets, the 5 tweets with the most favorites
mostpop <- head(tweets_only$text, 5)

#check the two objects, most_retweets and mostpop
#they are the same
most_retweets
mostpop
```
$$\\[.1in]$$
$$--------------------------------------------------------$$
$$\\[.1in]$$

Create a \textit{corpus} of the tweet content and put this into the object \texttt{Corpus} using the \texttt{tm} (text mining) package. (Hint: Do the assigned readings.)
```{r}
#install.packages("tm")
library(tm)

#Create a volitile corpus of all of the tweets in the tweets_only datatframe
Corpus <- VCorpus(VectorSource((tweets_only$text)))
```
$$\\[.1in]$$
$$--------------------------------------------------------$$
$$\\[.1in]$$
Remove extraneous whitespace, remove numbers and punctuation, convert everything to lower case and remove 'stop words' that have little substantive meaning (the, a, it).


```{r}
#remove all the whitespace from the tweets
TweetCorpus <- tm_map(Corpus, stripWhitespace)

#remove numbers
TweetCorpus <- tm_map(TweetCorpus, removeNumbers)

#remove punctuation
TweetCorpus <- tm_map(TweetCorpus, removePunctuation)

#Convert everything to lower case
TweetCorpus <- tm_map(TweetCorpus, content_transformer(tolower))

#remove stopwords, such as the, a, it.
TweetCorpus <- tm_map(TweetCorpus, removeWords, stopwords("english"))
```

$$\\[.1in]$$
$$--------------------------------------------------------$$
$$\\[.1in]$$
Now create a \texttt{wordcloud} to visualize the top 50 words the President uses in his tweets. Use only words that occur at least three times. Display the plot with words in random order and use 50 random colors. Save the plot into your forked repository.
```{r}
#To create a word-cloud:
##Note: followed steps laid out by https://towardsdatascience.com/create-a-word-cloud-with-r-bde3e7422e8a for manipulating TDM and creating word-cloud. Followed their code for this task.
#1. create a term document matrix
TDMTweets <- TermDocumentMatrix(TweetCorpus)
#2. remove sparse terms, because the object is too big otherwise
#the closer to 1, the more sparse the words are that are being removed
TDMTweets <- removeSparseTerms(TDMTweets, .99)
#3. turn it into a matrix
TDMTweets <- as.matrix((TDMTweets))
#4. sort the rows to be decreasing
TDMTweets <- sort(rowSums(TDMTweets), decreasing = TRUE)
#5. create a dataframe with the frequency count of each word
TDMTweets <- data.frame(word = names(TDMTweets), freq = TDMTweets)
#6. Create word-cloud
  #a. create pdf for word-cloud, saved to the repository.
pdf(file = "./wordcloud.pdf")
  #b. set seed for replicability
set.seed(1234)
  #c. install.packages("pals") for color palette
library(pals)
  #d. wordcloud function
    #use words that occur at least 3 times
    #word-cloud will have 50 most frequent words
    #display in a random order
    #set random.color to FALSE, so that colors will not be assigned by frequency
    #Have 25% of the words rotated 90 degrees
    #use the colors created above
wordcloud(words = TDMTweets$word, freq = TDMTweets$freq, min.freq = 3, max.words = 50, random.order = TRUE, random.color = FALSE, ordered.colors = FALSE, rot.per = .25, colors = kovesi.cyclic_mygbm_30_95_c78(length(TDMTweets$word)))
  #e. add a title, using mtext. Options side=3 is to place the text at the top of the page, cex=2 is the font size
mtext("Top 50 words tweeted by Trump in 2020", side = 3, cex = 2)
  #f. add a note to describe what the colors and words sizes mean. Use "\n" in text to move to a new line. Option side=1 places the text at the bottom on the page, cex=.5 is the font size.
mtext("Note: The colors do not add any information; \n they are only for aesthetic purposes.\n Text size indicates frequency of word used,\n with larger text meaning more frequent", side = 1, cex = .5)
  #e. end of pdf
dev.off()
```


$$\\[.1in]$$
$$--------------------------------------------------------$$
$$\\[.1in]$$
Create a \textit{document term matrix} called \texttt{DTM} that includes the argument \texttt{ control = list(weighting = weightTfIdf)}

```{r}
#subset tweets by only those that have been retweeted at least 1,000 times
retweeted <- tweets_only %>% 
  filter(retweet_count >= 1000)

#Turn this document into a corpus
RetweetCorpus <- VCorpus(VectorSource((retweeted$text)))
#Clean this corpus just like the one before
#remove all the whitespace from the tweets
ReweetCorpus <- tm_map(RetweetCorpus, stripWhitespace)

#remove numbers
RetweetCorpus <- tm_map(RetweetCorpus, removeNumbers)

#remove punctuation
RetweetCorpus <- tm_map(RetweetCorpus, removePunctuation)

#Convert everything to lower case
RetweetCorpus <- tm_map(RetweetCorpus, content_transformer(tolower))

#remove stopwords, such as the, a, it.
RetweetCorpus <- tm_map(RetweetCorpus, removeWords, stopwords("english"))

#Turn RetweetCorpus into Document-Term_Matrix
DTM_tfidf <- DocumentTermMatrix(RetweetCorpus, control = list(weighting = weightTfIdf, lowfreq = .8))
```
$$\\[.1in]$$
$$--------------------------------------------------------$$
$$\\[.1in]$$
Finally, report the 50 words with the the highest tf.idf scores using a lower frequency bound of .8.
```{r}
#turn the DTM into a dataframe using tidy()
#install.packages("tidytext")
library(tidytext)
DTM_tfidf_df <- tidy(DTM_tfidf)
#The tidy function turned my DTM into a data frame.
#Now the columns are: 1. the document the observation came from; 2. the term, which is word used in the original tweet (this is really the observation for practical purposes); and 3. the observation's count, which is the td.idf score.

#Get the 50 terms with the highest td.idf scores
#1. start with renaming the column name from count to tf_idf
DTM_tfidf_df2 <- DTM_tfidf_df %>% 
  rename("tf_idf" = "count") %>% 
  #2. then reorder the tf_idf column, so that its value is decreasing
  arrange(desc(tf_idf))%>% 
  #3 now remove all words that have http in them
  filter(!(str_detect(term, ".[(http)]."))) %>% 
  #4. keept only the distinct terms, in other words remove duplicate words
  ###Be sure to set .keep_all to TRUE to be able to keep all variables
  distinct(term, .keep_all = TRUE) %>% 
  #4. select only the top 50  words
  slice_head(n = 50) 

#print the top 50 terms with the highest td.idf scores
print(DTM_tfidf_df2$term)


```
 